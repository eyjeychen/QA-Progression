July 5, 2025
Why is Testing Necessary?
 From the user’s perspective:
It is everywhere
Software that has minor issues might cause huge problems and frustrations
Can cause:
Money loss
Waste of time and effort
Business reputation
Loss of life
No issue is small

From the project management perspective:
It is a form of quality control

Typical Objectives in Testing:
Evaluating work products such as user requirements, user stories , designs and code
Causing failures and finding defects
Ensuring required coverage of a test object
Reducing the risk level of inadequate software quality
Verifying whether specified requirements have been fulfilled
Verifying that a test object complies with contractual, legal, and regulatory requirements
Providing information to stake holders to allow them to make informed decisions
Building confidence in the quality of a test object
Validating whether the test object is complete and works as expected by the stakeholders
Test objectives  are usually straight forward 
Test objectives display a list of time and ask what is a test objectives and what is not

Validation vs Verification
Validation mentions the user more and asks if we are building the right product
Verification mentions the documents more and asks if are we building the product right

Errors, Defects and Failures
A person makes an error that creates a defect / bug in the software that can cause a failure in the system operation
Example:
Someone has a fever (failure, a symptom you can see) and goes to the Dr who decides something is wrong with the stomach (a defect) because the patient ate too much (error)
Failures can also be caused by environmental conditions

False-positive vs False-negative
False-positive
We did something wrong which we thought we found a defect but it was a wrong finding
Probably duet to:
Errors in the test execution
Defects in the test data
Test environments
Other reasons
False-negative
We did something wrong but we cannot find the defect

**August 2, 2025**

**Decision Table Testing**

- Different combinations of conditions produce different outcomes or actions
- It is a “combinatorial test techniques”
- A decision table lists all the input conditions at the top of the table and all the actions of the system at the bottom of the table
- Decision Table Testing in ISTQB Exams:
1. One example might give you a table and a situation of a specific combination of the conditions will ask you what the action will be
2. Some might give ready made test cases and ask you what are the remaining needed test cases to achieve full coverage
3. Hardest will be and ask you to build the table and determine how many test cases are needed
- **Coverage for decision table**: # of decision rules tested by at least one test case/ total number of decision rules
- Decision table testing helps to identify all important combinations of decisions
- Risk-based approach: eliminate the rules that are less risky if we didn't exercise them

**State Transition Testing**

- State transition techniques is concerned with systems that may exhibit different response based on depending on current conditions or previous history
- State transition diagram: shows possible software states as well as how the software enters and exits between states
- A state table records all possible both valid and invalid transitions and their effect on all the possible states
- Different State Transition Coverage:
1. All state coverage
- Test cases must ensure all states are visited
- **Coverage** is measured by: # of visited states/ total number of states
1. Valid Transitions Coverage
- Also called “zero switch coverage”
- Test cases must exercise all the valid transitions
- **Coverage is # of exercised valid transitions/ total valid transitions**
1. All Transitions Coverage
- Includes invalid transitions
- Its easier to use a transition table to calculate the all transitions coverage
- **Coverage is # of valid invalid and invalid transitions exercised by TCs / total number of valid and invalid transitions**
- **State transition testing** is testing only one invalid transition in a single test cases helps to avoid fault masking
- **Fault masking** is where one defect prevents the detection of another defect
- A **trigger** is an event that may cause a transition
- A **guard** is a boolean condition that permits or blocks the transition
    - If guard is TRUE then transition OK
    - If guard is FALSE then transition BLOCK
    - Is usually represented within brackets after an event name
- **Transition behavio**r is an activity that executes while the transition takes place

**Note:**

State transition testing is widely used within the embedded software industry
Root Cause Analysis 
Helps in preventive measures for the software in the near future

Root Cause, Error, Defect, Failure
Root causes leads to human error which then leads to the defect thus the failure
Example:
The designer is tired (root cause)
Designer documents wrong for disabled users (error)
The programmers are in a severe time pressure (root cause)
So they did not include exception handling for calculations (error)

Dynamic and Static Testing
Dynamic testing is testing that occurs once the system is already there, so expect that there will be defects
Static testing takes place early in the software life cycle
Includes techniques such as reviewing documents and prevents defects from being introduced into the code

**July 6, 2025**

**Verification and Validation**

- Verification is whether the system the specified documents
- Validation checks whether the system will meet user and other stakeholder needs
- Verification + validation = doing the right thing in the right way

**What is Testing?**

- Testing is a big industry and has many branches
- Set of activities to discover defects and evaluate the quality of software artifacts
- **Artifacts are anything the development and testing teams produce to help create the software product**
- **Test objects are any artifact that is being tested**

**Testing and Debugging**

- Testing finds defects
- Debugging fixes these defects (a development activity)
- Debugging activities include:
    - Reproduction of a failure
    - Diagnosis (finding the root cause)
    - Fixing the cause
- Confirmation testing or re-testing is to test to check whether the bug was fixed
- Static testing points to the defect directly, debugging is concerned with removing it
    - No need for reproduction or diagnosis since static testing directly finds defects and cannot cause failures

**Does testing increase the quality of the software?**

- **No. Fixing bugs is a development activity not a testing activity. So testing simply builds confidence in the quality of the test object by providing higher-quality test objects**

**Testing’s Contributions to Success**

- Provides a cost-effective means of detecting defects

**Quality Assurance and testing**

- **Quality assurance -** “process-oriented”. The better the process, the better the software. “Process improvements”
    - If a good process is followed correctly, then it will generate a good product
    - A preventive approach
    - Test results in QA provide feedback on how well the development and processes perform
- **Testing -** “product-oriented”. It is a major form of **quality control**.
    - A corrective approach
    - Test results for QC are used to fix defects

**July 8, 2025**

**The Concept of Coverage in Software Testing**

- Test coverage is an essential part of software testing
- Measures the amount of testing performed by a set of tests
- Test coverage measures the effectiveness of our testing
- Parts we can measure for coverage
    - Requirements Coverage = test against all requirements
    - Structural coverage = has each design been exercised during testing
    - Implementation coverage = has each line of code been exercised?
- How can we know which requirement has been tested?
    - Traceability matrix

**The Seven Testing Principles**

1. Testing shows the presence of defects, not their absence
    1. There is no such thing as bug-free software
2. Exhaustive testing is impossible
    1. Testing everything is impossible except for trivial cases
3. Early Testing Saves Time and Money
    1. Early testing would be cheaper
    2. Time and Effort is reduced if discovered in the early phase
4. Defects Cluster Together
    1. A small number of modules usually contains most of the defects
    2. Closely related to the pareto principle (80/20 principle)
        1. 80 percent of the problems are usually found in 20% of the modules
5. Tests wear out
    1. If same tests are repeated, eventually the same set of test cases will no longer find any new defects
6. Testing is context-dependent
    1. Different testing is necessary for different circumstances
    2. Testing in an agile is different in a sequential
7. Absence of errors is a fallacy
    1. A mistake that there is no error present

**Test Conditions, Test Cases, Test procedure, and Test Suites**

- **Test Condition** is an item or event of a component that can be verified by one or more test cases.
    - Example: a function, transaction, feature, quality, etc
- **Test Cases is a set of input values, preconditions, expected results and post conditions developed from a test condition or objective**
    - **High-level test cases** mean they don't indicate exact data, just logical information
    - **Low-level test cases are also called concrete test cases (e.g. = test with input 10). Are test cases that contain data**
- **Test Procedure  - steps to execute a test case**
- **Test Suite -  Categorizing test procedures in such a way they match your planning and analysis needs.**

**Test Activities, Testware and Test Roles**

- ISO/IEC/IEEE 29119-2 describes the test processes in ISO Standard
- Testing is a process not simply just testing

**Test Activities and Tasks**

- Test process consists of the 7 groups of activities:
1. Test Planning
2. Test Monitoring and Control
3. Test Analysis
4. Test Design
5. Test Implementation
6. Test Execution
7. Test Completion
- Each activity may contain several more activities which may contain one or more tasks
- Test activities are organized and carried out differently in different life cycles
- Are usually implemented iteratively, not sequentially

**1. Test Planning**

- Define the objectives of testing
- Decide what to test
- Who will do the test
- How will they do the testing
- Define specific test activities to meet the objectives
- Define when we can consider the testing complete, called the **exit criteria**

**2. Test Monitoring and Control**

- The ongoing activity of comparing actual progress against the test plan using test monitoring metrics defined in the test plan
- Here is where we evaluate the exit criteria
- Evaluating exit criteria is the activity where test execution results are assessed against the defined objectives

**3. Test Analysis**

- Knowing what to test and breaking it into test conditions
- Any info or documentation we have is analyzed to identify testable features and define test conditions
- **Test Basis  - any document we can use as a reference**
    - Requirements specifications, such as BRD, SRS, user stories, use cases, etc.
    - Design and implementation information, UML, UI, etc
    - Code itself
    - Risk Analysis reports
- Identify features and sets of features to be tested
- Define and prioritize the test conditions for each features based on analysis
- Capture traceability = meaning we have test conditions for all features we have decided to test

**4. Test Design**

- Test design answers the question how to test
- Designing sets of test cases and prioritizing them
- Identifying test data to support test conditions
- Designing the test environment
- Capture bi-directional traceability

**5. Test Implementation**

- The testware necessary for test execution is created during test implementation
- “Do we now have everything in place to run the tests?”
- Create and implement test procedures during test implementation
- Prioritizing the test procedures
- Creating the test suites and arranging them
- Building the test environment
- Prepare and implement test data
- Verifying the bi-directional traceability

**Note:**

Test conditions = test analysis

Test Cases = test design

Test procedures = test implementation

Design data = test design

Implement the data = test implementation

**6. Test Execution**

- Test suites are run during the execution following the schedule
- Keeping a log of the testing and testware (pass or fail)
- Run test cases in order manually or automated
- Comparing actual results with expected results
- Analyzing anomalies when there is a difference between actual and expected results
- Reporting defects to devs for fix
- Confirmation testing
- Verifying and updating traceability

**7. Test Completion**

- Occur at project milestones
- Collect data from completed tests to consolidate experience
- Checking deliverables if they have been delivered
- Documentation is in order
- Create test summary report
- Check whether all defect reports are don
- Make sure delete confidential data
- Handing over the testware to the maintenance team
- Analyzing lessons
- Using data for improvement of test process maturity

**July 11, 2025**

**Test Process in Context**

- Not performed in isolation
- Organizational software development processes typically include testing activities
- Stakeholders provide financial funding for testing
- Test depends on context
- Factors Influencing Test Processes:
1. Stakeholders
2. Team Members
3. Business Domain
4. Technical Factors
5. Project Constraints
6. Organizational Factors
7. SDLC
8. Tools

**Testware**

- Any output work products like documents, reports, and lists created as part of the test processes
- Test process organization variation implementation:
1. Types of work products created per test process
2. Ways those work products are organized and managed
3. Names used for these work products
- Testing Standards:
1. ISO/IEC/IEEE-29119-1
- Software Testing Concepts
1. ISO/IEC/IEEE-29119-2

-	Software Testing Process

1. ISO/IEC/IEEE-29119-3
- Test Work Products
- 7 Different Work Products Per Different Test Process

**Traceability Between Test Basis and Test Work Products**

- Traceability of the test cases to requirements can verify if the requirements are covered by test cases
- Good Traceability Supports:
1. Analyzing Impact Changes
2. Meeting IT governance criteria
3. Making testing auditable
4. Improving the understandability of test progress reports and test summary reports
5. Relate IT terms and testing to stakeholders so they can understand them
6. Providing information about progress
- **Note:**
    - Traceability has nothing to do in any kind of estimation
    - Also has nothing to do with selecting data
    - It also cannot determine risk level using traceability

**Roles in Testing**

- 2 Main Roles
1. Test Manager
- Test process management
- Test planning and monitoring, etc
- “How-to-do” things
- **Test processes: Test planning, monitoring and control, Test completion**
- Test coach -> outside of the team
1. Tester
- Overall responsibility is the technical aspect of testing
- “Hands-on” to do things
- **Test processes: Test analysis, Test design, Test implementation, Test execution**

**Essential Skills and Good Practices in Testing**

- Knowledge + Practice + Aptitude (natural talent)
1. Testing Knowledge
2. Thoroughness, carefulness, etc, attention to detail
3. Good communication skills
4. Analytical Thinking
5. Technical Knowledge
6. Domain Knowledge

**Communication Skills for Testers**

- Developers treat their codes as if its their kids. They don't take criticism lightly

**Whole Team Approach**

- Ability for a tester to work in a team
- Contribute positively to team goals
- Any team member with the necessary knowledge and skills can perform any task and everyone is responsible for quality
- The team members share the same workspace
- Team takes responsibility in all kinds of testing tasks
- It generates team synergy that benefits the entire project
- Improve team dynamics
- Improve communication between team members

**Independence of Testing**

- Independence is not a replacement for familiarity
- Degrees of Independent Testing (from dependence 1 to independence 5)
1. The developer
2. Tester from the developer team
3. Independent test team
4. Customer of specialist testers
5. Outsourced test teams or testers
- Pros:
1. Independent testers are likely to recognize different kinds of failures compared to developers
2. They can verify, challenge or disprove assumptions
- Cons:
1. More isolation from the development team
2. Developers may lose a sense of responsibility for quality
3. Maybe seen for bottleneck or blamed for delays in release
4. May lack important information

July 13, 2025
Section 2 talks about:
How testing is incorporated in other dev activities
Different Levels and types of testing
Maintenance Testing
SDLC Models
How activities are organized to create the software
Vary according to:
Objective
Discipline
Interest
Time to market
Documentation
Types of activity performed at each stage
Others
Testing depends on the SDLC model
Categories of SDLC Models
Sequential Development Models
Linear with sequential flow of activities
No overlaps of phases
Iterative &Incremental Development Models
Short cycle of activities that can be revisited any time
Sequential Development Models: Waterfall
Oldest and well known
Each stage must be completed before moving on to the next stage
Requirements Phase
From the user
Systems specification document
Functional specifications
Design Phase
Global design or architecture design or high-level design to map the system requirements
Detailed design or low-level design: very specific about the functions
Coding Phase
Developers code the detailed design of each module to software
Testing Phase
Towards the end of the project lifecycle
*Waterfall model is only a good model for very specific circumstances
July 14, 2025
V-Model
A sequential model
Testing needs to begin as early as possible in the lifecycle
The model integrates testing throughout the entire development phase
The 4 Test Models in V-model
Component Testing
Search defects in implemented components
Integration Testing
Test interfaces and interactions between different components
System Testing
Behavior of the whole system
Acceptance Testing
Conducted whether or not to accept the system
E.g client UAT
**July 15, 2025**

**Iterative Incremental Development**

- Can be separated
- Incremental = building a system in pieces
- Iterative = built in a series of cycles where every piece can be used unlike incremental
- In iterative and incremental software, each cycle involved with everything we have in an SDLC
- Customers are heavily involved
- Examples of Iterative - Incremental models:
1. Rational Limited Process = each iterations are long and the increments are large
2. Spiral/Prototyping = creating experimental increments
3. Scrum = part of agile. Each iteration is short
4. Kanban = part of agile. Implemented with or without a fixed length of iterations
- Regression testing is increasingly important as the system grows
- Automation testing should be considered for faster testing in each cycle

Good Testing Practices in SDLC

1. In each development activity there is a corresponding testing activity
2. Each test level has specific objectives
3. Early testing in analysis and design; testing for every test level should begin early
4. Testers should be involved in reviewing documents or work products as soon as a draft is available

**What is Agile?**

- Ability to create and respond to change for good business development
- Ability to quickly reprioritize use of resources
- Not a project management methodology
- Not a development framework or lifecycle
- It is a philosophy
- It is lightweight

**Manifesto for Agile Software Development**

1. Individuals and interactions over process tools
2. Customer collaboration over contract negotiation
3. Working software over comprehensive documentation
4. Responding to change over following a plan

**Scrum**

- Most agile development framework
- Instruments and Practices of Scrum:
1. Sprints
- Fixed length
1. Product Increment
- Each sprint produces a product called an implement
1. Product Backlog
- Prioritized list of planned product increments in a log
1. Sprint Backlog
- Selects high priority list from product backlog and puts it into sprint backlog
1. Timeboxing
- Short fixed duration of periods of time in which activities are implemented
- Daily stand up meetings in 15 mins in 2 weeks (timeboxed)
1. Transparency
- Reporting of progress

Scrum Roles:

1. Scrum Master
- Like a project management head
- A coach
1. Product Owner
- Represents the customer which manages the product backlog
1. Development Team

**Impact of the Software Development Lifecycle of Testing**

1. Scope and timing of test activities
2. List of details of test documentation
3. Choice of test technologies and test approaches
4. Extent of test automation
5. Roles and responsibilities of a tester

**Testing as a Driver for Software Development**

- TDD, ATDD BDD approaches follows the test-first approach since the tests are defined before the code is written
- Supports software development module
1. TDD (Test-Driven Development)
- Tests are written before the code is written
- Directs coding through test cases instead of extensive software design
1. ATDD (Acceptance Test-Driven Development)
- Define the acceptance criteria and tests during the creation of user stories as part of the system design processes
1. BDD (Behavior Driven Development)
- Testing the code based on the expected behavior

Differences between the 3:

1. TDD
- Concentrates more on developer
- Developer makes unit tests to make sure the code works
1. ATDD
- Involves the customer from the beginning
1. BDD
- Defines the expected behavior of the system so anyone can understand
**July 16, 2025**

**DevOps and Testing**

- Isolation Issues:
1. Slow Software Delivery
2. Frequent errors and downtime
3. Inefficient processes
4. Limited visibility
5. Resistance to change
- Devops is a combination of development and operations
- CI/CD = continuous integration and continuous delivery
- Combination of these teams uses practices to automate processes
- Use a tech stack and tooling that helps them operate and evolve apps
- Introduces collaborative automated and approach to software delivery
- Promotes team autonomy, fast feedback, integrated tools and technical practices like CI and CD
- Benefits of Devops on Testing:
1. Fast feedback on the code quality
2. CI promotes shift-let approach
3. Automated Testing: devops promotes this
4. Increases view on non-functionality quality characteristics (performance)
5. Efficient regression testing
- Cons of Devops on Testing:
1. Devops delivery pipeline must be defined and establish
2. CI/CD tools must be introduced and maintained
3. Test automation requires additional resources

July 17, 2025
Shift-Left Approach
Principle of early testing
Does not wait for code to be implemented before testing
Shift-Left approach does the following:
Review specification
Write test cases before code is written
Continuous integration and continuous development
Begin implementing an automated process
Perform non-functional testing on component test-level
Retrospectives and Process Improvements
Does not include the end users
Retrospectives are frequent process improvements
Done at the end of each iteration
Notes what is and is not successful
Benefits of Retrospectives in Testing:
Increased effectiveness and efficiency
Increased quality of testing
Improved quality of test basis
Better collaboration
Test Levels
Groups of test activities
Unit testing-> component testing -> component integration testing -> system testing -> system integration testing -> acceptance testing
Characterizing Test Levels:
Specific Objectives
Test basis
Test objects
Specific approaches and responsibilities
Typical defects and failures
The 5 Main Test Levels:
Component Testing
Test components in a system
Testing unit per unit or component by component
Component Integration Testing
Testing interactions between components
System Testing
Overall behavior of the entire system
System Integration Testing
Several interfaces of a system
Acceptance Testing
Validation and readiness for deployment

**July 18, 2025**

**Component Testing**

- Lowest level of testing
- Small pieces of code developed by developers
- Unit or module testing
- Tested in isolation
- Done by developers
- Objectives of component testing
    - Reducing risk of delivery a bad component
    - Finding defects in the component
- Examples of Work Products that can be used as a test basis for component testing
    - Detailed design
    - Code
    - Data Model
    - Component Specifications
- A stub is a dummy code used in a place of a cold component
- A driver is a dummy code used in place of a collar component
- Examples of typical defects and failure in Component Testing
    - Incorrect code and logic
    - Data flow problems
    - Incorrect functionality

**Component Integration Testing**

- Putting units or components together to build the system
- Testers concentrate solely on the integration itself
- Examples of Work Products that can be used as a test basis for Component Integration Testing
    - Software and system design
    - Sequence diagrams
    - Uses cases
    - Architecture at component or system level
    - Workflow
- Usually part of the continuous integration process
- Focuses on the interactions and the interfaces between integrated components
- Usually carried out by developers and generally automated
- Examples of typical defects and failure in Component Integration Testing
    - incorrect or missing data
    - Incorrect sequencing or timing of interface calls
    - Interface mismatch
    - Failures in communication between components
- Larger integration or the “big bang” where all components are integrated can leave to problems as it is harder to isolate which component caused the defect
**July 19, 2025**

**System Testing**

- Whole system
- E2E testing both functional and non-functional
- Test environment should correspond to the final target or production environment
- Work Products used:
    - System Requirements Specification
    - Use Cases
    - Epics and user stories
    - Models and system behavior
    - Risk Analysis Reports
    - System and user manuals
- Test Objects
    - Applications
    - Hardware/Software Systems
    - Operating Systems
    - Systems Under Test (SUT)
    - System configuration and configuration data

**System Integration Testing**

- interactions /interfaces between systems
    - Example is web services or external system
- Objectives are the same for component integration testing

**Acceptance Testing**

- Yes or no testing; “should we release the system?”
- Behavior and capabilities of the whole system
- Produce information to assess the system’s readiness for deployment and use by the customer or end-users
- Typical Defects and Failures
    - System workflows do not meet user requirements
    - Business rules are not implemented correctly
    - Non-functional requirements
    - Does not satisfy contracted or regulatory requirements
- Common forms of Acceptance Testing
1. User Acceptance Test
- Intended users
1. Operational Acceptance Test
- System administrator’s point of view
1. Contractual and Regulatory Acceptance Test
- Users or independent users
- Government and legal regulations
1. Alpha and Beta Testing
- Alpha is inviting potential users in own developer site
- Beta testing is inviting potential users but they test in their own locations

**Test Types**

- Group of test activities aimed at testing specific characteristics of a software system
    - Functional Testing
    - Non-functional Testing
    - Black-box testing
    - White-box testing

**Functional Testing**

- “What” it does
- Should be performed at all test levels
- Objectives of functional testing
1. Functional Completeness
- Functions covers all the specified tasks and user objectives
1. Functional correctness
- Provide correct results
1. Functional appropriateness
- Fulfill the needs of the user

**Non-functional Testing**

- “How well it behaves”
- Eg. performance testing, security, maintenance, flexibility
- ISO/IEC 25010 for quality characteristics
- Performed at all test levels and is done as early as possible

**Black-box Testing**

- Specification-based testing
- Testing w/o looking at its internal code or implementation details
- Derives test cases and test scenarios from external documentation such as req. Specification and design documents

**White-box Testing**

- Testing w/ knowing how the system is structured
- Cover underlying structure of the system by tests

**Relationship b/w test types and test levels**

- Unnecessary for all software to have every test type represented
- Important to run appropriate test types at each level

**Confirmation and Regression Testing**

- Confirming if the defect has been fixed
- Regression testing helps to find out if the fix has introduced a new defect somewhere else
    - Test areas that was already tested
    - Performed at all test levels so its better to be automated
- Regression test sample:
- 

|  | EX 1 | EX 2 | EX 3 |
| --- | --- | --- | --- |
| TC 1 | (1) F | (4)P | (7) P |
| TC 2 | (2) P | (5)  F | (8)  P |
| TC3 | (3)F | (6)F | (9) P |
- Regression test: 5 and 7
- Confirmation test: 4, 6, 8, 9
- If we see a passed test and repeat it on the next execution then it is a **regression test**
- If we see a failed test then executed again then it is a **confirmation test**

**Maintenance Testing**

- Perform impact analysis before a change is made
- Takes place in a live system or a system in production
- Testing when any changes are made as part of maintenance

**Triggers for Maintenance and Maintenance Testing**

1. Modification
- Enhancement, upgrades
1. Upgrades or Migration
- One platform to another
- Data conversion
1. Retirement
- When an application reaches the end of its life

**Impact Analysis**

- Determining how the existing system maybe affected by changes
- Used to help decide how much regression testing is needed to do
- Identify impact of a change on existing tests

**July 22, 2025**

**Static Testing Basics**

- Is testing without executing code
- First line of defense against defects
- Relies on examining the software products
- Involves manual reviews and static analysis
- Static Analysis
    - Essential for safety-critical computer systems
    - Part of security testing
    - Incorporated into automated tests
    - Helps to identify problems prior to dynamic testing
- Static Analysis Test Objectives
    - Enhancing quality
    - Finding Bugs
    - Evaluating features
- Involves review techniques
    - Ensures user stories are complete and understandable

**Differences between static and dynamic testing**

- Static:
    - Finds defects
    - Improve consistency, and internal quality
    - Can be applied to non-executable work products
- Dynamic:
    - Finds failures
    - Focuses on externally visible behaviors
    - Can only be applied to executable code
- Typical Defects found in static testing:
1. Requirements Defects
2. Design Defects
3. Coding Defects
4. Deviations from standard
5. Incorrect interface specification
6. Security vulnerabilities
7. Gaps and inaccuracies in test basis
**July 23, 2025**

**Work Products that Can Be Examined by Static Testing**

- Any written work product can be reviewed by static testing
    - Specifications
    - Epics, user stories, acceptance criteria
    - Architecture and design specifications
    - Source codes
    - Testware
    - User guides
    - Web pages
    - contracts , project plans, etc
    - models
- A compiler - a static analysis tool
    - Example: grammar checking tool
- Some work products that are not understandable or readable by human language are not able to be identified by static testing
**July 26, 2025**

**Work Products that Can Be Examined by Static Testing**

- Any written work products that a human can withstand
- Specifications
- Epics, user stories and acceptance criteria
- Architecture and design specifications
- Source Code
- Testware
- User guides
- Web Pages
- Contracts, projects plans, schedules, budget
- Models, such as activity diagram

**Benefits of Static Testing**

- Early detection of defects
- Identification of defects
- Preventing Defects
- Increasing development productivity
- Reduced development timescales due to lesser bugs
- Reduced testing time and cost
- Reducing total cost of quality
- Improving communication between members in reviews
- Evaluate quality and build confidence in work products

**Benefits of Early and Frequent Stakeholder Feedback**

- Prevent misunderstanding about requirements
- Ensure changes to requirements are understood and implemented earlier
- Helps development teams to improve understanding
- Allows teams to focus prioritizing these features

**Work Product Review Process**

- Informal reviews do not follow a defined process
- Formal reviews follow procedures
- ISO ECE 20246 is a review process
- Five Groups of Activities in the Review Process
1. Planning
- Defining the scope
- Estimating effort and timeframe
- Identifying review characteristics
- Selecting people who will participate in the review
- Define entry and exit criteria
1. Review Initiation
- All reviewers should know what they are reviewing
- Distributing the work produced
- Explaining the scope
- Answering questions
1. Individual Review
- Each of the participants will review alone
- Noting potential defects
1. Communication and Analysis
- Communicate identified potential defects
- Evaluate quality characteristics
- Evaluate review findings
1. Fixing and Reporting
- Defect reports

**Roles and Responsibilities in Reviews**

1. Author
- Creates work product for review
- Fix defects in the created work product
1. Manager
- Review Planning
- Execution of reviews
- Assign staff, budget and project schedules
- Control decisions made
1. Moderator or Facilitator
- Effective planning and review meetings
- Ensures no bug fixing will be discussed in the review meeting
- Mediates
1. Review Leader
- Overall responsibility for the review
- Decides who will be involved and organizes the when and where
1. Reviewers
- Subject matter experts
- People in the projects
- Potential defects on the work products
1. Scribe/Recorder
- Collects potential defects found in the review meeting

**Review Types**

- Focus of any review depends on the agreed objectives of the review
- Review types depend on:
1. SDLC Model
2. More mature process = more formal review
3. Complexity of the work product
4. Legal on regulatory requirements
5. Need for audit trail
- Types of Review:
1. Informal Review
- “Pair review”
- Quickly find defects or quickly solve process
- Not usually documented for review
- No process and is quick
1. Walkthrough
- Author walkthroughs the document
- Meeting is led by the author
- Main purpose is learning and gaining understanding
1. Technical Reviewing
- Focuses on achieving consensus on a technical aspect
- Evaluating work
- Reviewers are experts in their field
- Review meeting is optimal
- Scribe is mandatory
- Define lofs
- Keyword: led by a moderator
1. Inspection
- Most formal type of review
- Purpose: detecting potential defects
- Scribe is mandatory
- Led by a trained moderator
**July 28, 2025**

**Introduction to Test Analysis and Design**

**Test techniques**

- Each test techniques tackles a different situation
- Helps the tester with test analysis (what to test) and test design (how to test)
- Define test conditions, test data and identify coverage items
- Effective Testing
    - Find more faults
    - Focus attention on specific types of faults
    - Know you're testing the right thing
- Efficient Testing
    - Find faults with less effort
    - Avoid duplication
    - Systematic techniques are measurable

**Categories of Test Techniques**

1. Black-box Techniques
- Tests are derived from a specification of a system
- “behavior-based “ techniques meaning you only know how the system should behave
- Applicable to both functional and non-functional
- Concentrate on input and output without reference ot its internal structure
1. White-box Techniques
- Deriving test cases directly from the structure of the system
- “structure-based”
1. Experience-based Techniques
- Based on the stakeholder’s experience
- Depends heavily based on the tester’s skills

**Black-box Test Techniques**

- Characteristics of Black-box:
    - Test conditions, test cases, etc are derived from a test basis that may include software requirements, specifications, etc
    - EP, BVA, Decision Table Testing, State Transition Testing
**July 29, 2025**

**Equivalence Partitioning**

- Based on the idea that inputs are divided into partitions that exhibit similar behavior
- Example of equivalence partitions:
    - Age < 20
    - 20 <= age <= 50
    - Age > 50
- Example of equivalence partitions:
- 
    
    [](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdCn92HbZT-A-lA9vqWaa3oI3Gug_e3RWKHhMQ-aEVLNGma83zKjdMmbkICtjyapU6IpxYJ56cOZLm-St21CkVRrXw5zd3t4CXPTOGP6HSy258v7Zjvji7qtZrANYlQ6x4eGU_w?key=s7mNb_USJyD67HQorXBaxA)
    
- Valid values = values should be accepted by the system, vice versa is invalid values
- Each value must belong to one and only one equivalence partition
- One value from each partition should be selected
- When invalid partitions are used in test cases, they should be tested individually
- Can be identified for any data element related to the test object
- Any partition may be divided into sub-partitions
- Coverage is measured as the number of equivalence partitions tested by at least one value divided by the total number of equivalence partitions
- Example:
    - 1. AB36P (covers partitions 1, 4,6)
    - 2. Very Long (Covers partition 3)
    - **Coverage: 4/7 x 100 = 57%**
- In creating partitions, ask yourself “**would this value change the way the software behaves?”**
- If **YES,** then every value for the input should be in a separate partition
- If **NO**, then all the values can reside in a single partition
**August 1, 2025**

**Solving Equivalence Partitions**

- Understand to use only one value from each partition unless you have to
- If there is an answer option that users more than one value from the same partition it is most likely to be a wrong answer

**Boundary Value Analysis**

- Error tends to cluster around boundaries
- Works hand in hand with EP
- BV is the min max values of a partition
- Example:
- Given a range of 20 to 50
    - 2 value BVA = 19, 20, 50, 51
    - 3 value BVA = 19, 20, 21, 49, 50, 51

**Note:** in this day I focused on the questions for EP and for BVA, for august 2 that will be the same process


August 6, 2025
White box test techniques
Also called structure-based testing
Since you have information on the structure of the test object
Info such as how the database and system is built, modules are organized and access to the source code itself
Example: 
Age is used in a byte so you'll use values outside of a byte

Characteristics of white box techniques
TCS are derived from a test basis that may include anything related to the structure of the software 
Concept of coverage is heavily used. Measured based on the items within a selected structure
Specifications are often used as an additional source of information to determine expected outcome of test cases
Can be used at all test levels
White box testing techniques are used to assess the amount of testing performed by tests 
Statement Testing and Coverage
Statement testing exercises the executable statements in the code
Statement coverage is determined by the number of executable statements covered by the executed test cases/ Total number of statements
Statement coverage is the weakest white-box techniques
In statement coverage, size of the test cases do not matter
Statement coverage can't tell us anything about how many statement in the code cant be executed
100% coverage means every executable statement was executed at least once
August 9, 2025
Branch Testing and Coverage
Does not look at the code from the statement’s point of view
Used to test every possible branch in the control flow graph of a program
Every branch in the code is executed at least once
Example: an IF statement has 2 branches (one branch for TRUE, one branch for FALSE outcome
For a SWITCH statement, there is a branch for all the possible case
To get the total number of branches = multiply total number of IF statements by 2 (2 IF Statements = 4 branches)
Statement testing may provide less coverage than branch testing
Branch coverage helps to find defects in code where other tests have not taken both true and false outcomes
100% branch coverage guarantees 100% statement coverage but not vice versa
The Value of White-box testing
Is able to detect some of the intended features by understanding the logic used to implement such features
Can be used in static testing 
Can be used in dry runs of the code 
Well suited to review code that is not yet ready for execution 
Can be used as a coverage measurement technique
Adding more test cases to increase coverage
Experience-based Test Techniques
Error guessing
Exploratory Testing
Checklist-based
Are derived from the user’s and tester’s skil land intuition 
Helps to identify tests that were not easily identified by other systematic techniques
May not be measurable by coverage
Derived from a test basis that include tester’s and user’s experience
Error Guessing
Used to anticipate the occurrence of errors ,defects and failures based on the tester’s knowledge
Items such as how has the application worked in the past? What types of mistakes do the developers tend to make?
Not intended to be used by itself but rather support other test techniques
Effectiveness of the error guessing technique varies heavily on the tester’s experience
Fault attack = enumerate possible list of faults and defects


**August 15, 2025**

**Exploratory Testing**

- Use the tester’s experience to test the software w/o going into the formal cycle of writing test conditions, test cases and test procedures
- Keyword is “simultaneously” create all of these (writing test conditions, test cases and test procedures)
- More effective if the tester has experience
- Conducted using “session-based testing” to structure the activity or meaning fixed duration
- Uses a **test charter** which contains test objectives which focuses on critical areas
- **Test charter can be produced as part of the test analysis**
- Use **test session sheets**  to document the steps followed and the discoveries made
- Is most useful when there are few or inadequate specifications or significant pressure on time

**Checklist-based Testing**

- Like a to-do list
- In this testing, a tester designs, implements, and executes tests to cover test conditions from a checklist
- Checklists can be generic or specialized
- Should not contain items that are checked automatically, items better suited as entry/exit criteria, items that are too general
- If checklists are high-level, some variability in the actual testing is likely to occur so it leads to better coverage


**August 16, 2025**

**Collaboration-based Test Approaches**

- Avoid getting defects in the first place
- Defect avoidance by collaboration and communication

**Collaborative User Story Writing**

- Specification problems can result from a user’s lack of insight in their needs
- In collaborative user story writing requirements are captured from developers, testers and business representatives
- User Story
    - Written from the point of value of the user
    - Are small, understandable chunks of business functionality
    - Written in the following format as <role> I want <functionality> so that <business benefit>
    - Must address both functional and non-functional characteristics
- **INVEST technique in user story writing:**
    - Independent
    - Negotiable
    - Valuable
    - Estimatable
    - Small
    - Testable
- The 3Cs Concepts in User Story Writing
1. Card
- Physical media describing a user story
- Identifies requirements so it must be accurate since it will be used in the product backlog
1. Conversation
- Explains how the software will be used
- Can be documented or verbal
- Begins during the release planning phase
1. Confirmation
- Agreeing on the acceptance criteria so you know when your are done

**Acceptance Criteria**

- Part of the user story
- Provide developers and testers with an extended vision of the activities that business representatives will validate
- Conditions that the developers must implement to be accepted by the stakeholders
- Uses of Acceptance Criteria
1. Define the scope of the user story
2. Reach consensus among the stakeholders
3. Describe both positive and negative scenarios
4. Serve as a basis for the user acceptance testing
5. Allow accurate planning and estimation
- Should Address the ff: (Acceptance Criteria
1. Functional behavior
2. Quality Characteristics
3. Scenarios
4. Business Rules
5. External Interfaces
6. Constraints
- Design/implementations that restricts the options for the developers
1. Data Definitions
- Ways to Write Acceptance Criteria
1. Scenario-oriented
- given/when/then format in BDD
1. Rule-oriented
- Rules that are listed down briefly

**Acceptance Test-Driven Development (ATDD)**

- A test first approach where the test cases are created before implementing the user story
- To create ATDD test cases steps:
1. Specification Workshop
2. Create the test cases (can be manually or automated)
3. Positive test cases first
4. Negative test cases
5. Non-functional characteristics
August 30, 2025
Test Planning
•	Test strategy or test policy provides a generalized description of the test process, usually at the product or organizational level
•	The test strategy describes at a high level the 'how' of testing for an organization
•	The test approach is the implementation of the test strategy
•	'How we will test this project'
•	A test plan is how we will implement this test approach for this project
Purpose and Content of a Test Plan
•	A project plan for the testing work to be done in development or maintenance project
•	A test plan is used as a vehicle for communication with other members of the project team
•	Ensures there is a list of tasks and milestones
•	What to do, whom, how, by whom, etc
•	Purposes include:
•	Identifying objectives
•	Documents the needed resources
•	Documents processes
•	Documents the means or how to satisfy the test objectives and test schedule
•	Decide test results will be evaluated
•	Means of communications with team members
•	Guides the tester’s thinking
•	Test Plan Elements:
•	Context of Testing:
•	Defining the test objectives
•	Testing scope
•	Test basis
•	Assumptions and constraints of the test project:
•	Assumptions
•	Constraints
•	Stakeholders
•	Communication
•	Risk Register
•	Budget and Schedule
•	Test Approach
•	This is where to explain how we will perform the testing by defining the test levels, test types, test techniques, test deliverable, entry and exit criteria
Agile Planning vs. Traditional Planning
•	Agile projects have high degrees of uncertainty and risks
•	Therefore in agile projects, creating detailed upfront plans is problematic because the requirements in agile projects are progressively elaborated
•	Therefore, the planning in agile is performed in short intervals throughout the lifecycle
•	Difference between agile and traditional planning is that agile planning is done at a high level in the beginning of the project
•	Agile Planning Stages:
•	First level: product vision
•	Second-level: product planning
•	Release Planning and Iteration planning
Release Planning
•	Iteration: short development period
•	Release: group of iterations
•	Looks ahead to the release of a product
•	A release often takes a few months, say 3–9 months
•	Release plans are high level
•	Factors that may trigger release plans:
•	Delivery capabilities
•	Technical issues
•	The discovery of new markets and opportunities
•	New competitors
•	Business threats
Tester’s Contribution to Iteration and Release Planning
•	Release Planning: release of a product, serves as a basis for the test approach and test planning
•	Testers involved in Release Planning:
•	Participate in writing testable user stories and acceptance criteria
•	Participate in project and quality risk analyses
•	Estimate test effort associated with user stories
•	Determine the test approach
•	Planning the testing for the release
•	Testers involved in Iteration Planning:
•	Participate in the Detailed Risk Analysis of User Stories
•	Determine the Testability of User Stories
•	Break Down User Stories into Tasks
•	Estimate Test Effort for All Testing Tasks
•	Identify and Refine functional and non-functional aspects of the test object
August 31, 2025
Entry and Exit Criteria
Entry criteria or Definition of Ready define preconditions for undertaking a given test activity
Typical entry criteria include:
Availability of testable requirements
Test items that have met the criteria
Test env and readiness
Test tools
Test data
Budget
Initial quality level of a test object
Exit criteria are used to determine when a test activity has been completed or when it should stop
Typical exit criteria include:
Definition of done
Planned tests have been executed
Defined level of coverage has been achieved
Number of unresolved defects is within agreed limit
Defects are low evaluated levels of quality 
No of failed test cases
Static testing has been perfomed
All defects are reported
Regression tests are automated
Costs
Exit when the time is over
Entry and exit criteria should be defined for each test levlw and test type and will differe based on the test objectives
Estimation Techinques 
Estimate the effort needed to execute the plan
estimated effort is used to estimate other elements like the time needed, budget, resources
Based on assumptions and subject for error
Estimation for small taks are usually more accurate than the large ones hence why bigger tasks can be subdivided into smaller ones
Estimation categoies usually have:
Metric-based techniques
Expert-based techniques
Metric-Based Techniques
Collect data from previous projects or Iterations to estimate data (so much like historical artifacts for forecasting)
Depends if the accuracy of the collected data is close
1. Estimation Based on Ratios
Figures are collected from previous projects to derive “standard ratios” for SIMILAR projects
Example: In the previous project where it took 1000 hours to do and 300 hours of testing effort, in the new project that took 2000 hrs to do, we can estimate that the testing effort would take around 600 hours
2. Extrapolation
Makes measurements as early as possible in the project to gather data
With enough observations, the effort required can be estimated by extrapolating this data
Makes sense in iterative projects
Expert-based Approach
Depends on using the experience of some stakeholders to derive an estimate
Business experts
Test Process consultants
Developers
Analysts & Designers
Anyone with knowledge of the application to be tested or the tasks involved in the process
1. Wideband Delphi
Iterative
Expert-based technique
Experts make experience based estimations
Asking each expert to provide an estimate of the effort then collaborate to discuss each reasoning and finally asked to make a new estimation based from each other’s feedback
Helps build a complete task list
Helps eliminate bias in estimates 
More committed to estimates
Collaboration helps with eliminating bias
Recognize value of Iteration
Planning poker is a commonly used technique and is a variant
Estimates are usually made using cards with numbers representing the effort size
2. Three-point Estimation
Three estimations are made by the experts
"0" represents an optimistic estimate everything will progress smoothly 
"p” represents a pessimistic estimate assumes everything will go wrong 
"m”represents most likely estimate which is a mix between 0 and p 
"e”is their weighted mean
Beta Distribution or known as PERT Project Evaluation 1 Review Technique.  E=(0+4*M +P)/6.
Calculate Measurement Error Standard Deviation SD = (P-0) / 6
Benefits of Using 3-point estimation technique
Improved accuracy since it considers multiple scenarios 
Risk Assessment which helps assess risks 
Enhanced Decision-making 
Stakeholder communication for transparency More Realistic Planning with a chievable project goals 
Continuous improvement which helps identify areas of improvement
Test Case prioritization 
Commonly used strategies in prioritization are 
1.) Risk-based Prioritization 
order of test execution is determined by the results of risk analysis 
test cases with the most criticalor high-impact risks are executed first
2.) Coverage-based Prioritization
Focuses extent of coverage by the test cases 
the goal is to maximize coverage by executing test cases that have the highest coverage
3.) Requirements-based Prioritization 
assigning priorities to different requirements based on their importance to the stakeholders
Usually problematic if there are dependencies with other test cases who have lower prioritization. 
 Test Pyramid
Pyramid Layers Represent groups of tests. 
Higher the layer, lower test granularity or count of test cases and lower the test execution time.
Base of the pyramid = unit tests have a fine granularity small in size and is focused on testing individual components in isolation
Middle of the pyramid = integration tests. Fowses on verifying interactions & collaborations between components 
Top of the. pyramid=coarsest granularity: stimulate E2E user interactions. validates the system's overall behavior
Test Isolation 
means tests shouldn't rely on external systems, databases or services. 
unit tests have the highest isolation
efforts are made to minimize external dependencies & isolate the components being tested as much as possible
E2E tests are less isolated compared to lover level tests hence meaning “coarsest” granularity
Test Execution Time 
Unit tests at the base of the pyranid are fast to create since they don't involve the complexities of broader system interactions
Integration tests take longer to excute than unit tests but are generally faster than Ul tests 
E2E tests are the slowest to execute
Test Automation 
used to support test automation
How much effort . should be allocated to different lends of automation
unit A integration tests are automated & created using API based tools 
toplevels are automated - Using GUI-based tools
Test Coverage 
High-level tests are generally slower than the tests in the lower layers but they check a large piece of functionalty so just a few of them are I needed to achieve good coverage l 
Benefts of the test Pyramid 
Early detection of defects 
Optimizing Feedback Loop 
cost-efficiency 
Maintainability 
Focus on integration 
EZE confidence scalability 
Testing Quadrants 
Helps align test levels with appropriate test types & activities. 
Apply to dynamic testing rather than static testing
 Four Quadrants: 
1) Quadrant Q1
Unit-level 
Technology-facing 
supports developers 
contains component & component integration tests 
should be automated 
2) Quadrant Q2 
System- level 
Business-facing 
Confirms product behavior 
contains functional-tests 
check acceptance criteria 
can be manual or automated 
often created during user story deilopment 
useful when creating automated regression test suites
3.) Quadrant Q3 
system or user acceptance level 
Business-facing 
contains tests that critique the product 
contains exploratory testing, usability testing, UAT 
user-oriented 
often manual 
4) Quadrant Q4 
System or operational acceptance level 
technology-facing 
Contains tests that critique the product. 
contains smoke tests & non-functional tests like performance & security 
Often automated
Benefits of Testing Quadrants 
Comprehensive Test Coverage 
Balanced testing strategy 
Alignment with Agile Principles 
Improved communication & collaboration 
Early & Continuous Testing
Risk Mitigation 
Flexible & adaptable Testing Approach 
Continuous Improvement
Risk and Testing 
Risk is an important factor in the testing activity 
if risk is high = more effort in testing the software 
 Risk Definition and Risk Attributes 
Event in future has negative  consequence
Testing is used to reduce the probability of a negative event occurring or reduce it's impact 
Risk is used to focus on the effort required during testing. used to decide where & when to start testing 
Risk-based Testing 
 TCs are selected prioritized and managed based on risk analysis & control
Risk Management Activities Approach
1.) Analyze what go wrong (risks) 
2.) Determine which risks are important 
3.) Implement actions to mitigate those risks 
4.) Make contingency plans to deal with the risks
 Product & Project Risks
Product=software itself 
Project = activities needed to create the product
Product (Quality) Risks would be that a work product may fail to satisfy user needs
Project Risks are risks that may have a negative effect on a project's ability to achieve it's objectives
 Product Risk Analysis 
Risk Identification & Risk Assessment
Risk Identification 
one of the toughest activities 
Techniques in Risk Identification 
Brainstorming 
Interviews 
Risk workshop 
Risk templates & checklists 
calling on past experience 
cause-effect diagrams 
Risk. Characterization Factors 
Risk-likelihood = probability of the risk 
Risk Impact (harm) = consequences of this occurrence
Level of risk = probability x impact
Risk Assessment
Involves: 
Categorization of Identified risks 
Prioritizing risk 
proposing ways to handle them
Quantitative Approach = risk level is likelihood * impact 
Qualitative Approach=determined using a risk matrix (low, medium, high)
Product Risk Control
consists of risk mitigation & risk monitoring
Risk mitigation = implementing actions proposed in risk assessment to reduce risk level
4 ways to respond to risks: 
1.) Avoid=doing anything to make the risk level zero
2.) Mitigate = lower risk level
3.) Transfer = moving risk from your side to another side
4.) Accept -=actively put up a plan while waiting for the risk to happen
September 3, 2025
Test Monitoring, Test Control and Test Completion 
- Test monitoring is to gather information & provide feedback and visibility about test activities 
- Purpose/Purposes of test Monitoring: 
give feedback to test team & manager 
provide project team visibility about test results 
check if exit criteria is also satisfied 
gather data for use 
prove that the plan is right 
- Test Control is needed to get things back on track. It involves a guiding corrective action that is taken
-Test Control Actions Examples: 
Re-prioritization of tests due to risks. 
Changing test schedule due to availability
Re-evaluation of items for entry & exit criteria 
scope adjustment for testing due to change requests
Adding new resources 
-Test completion collects data from completed test activities
Metrics Used in Testing
test metrics are gathered to 
show progress against the test plan 
current quality of the test object 
effectiveness of the test activities with respect to the objectives or an iteration
 Example of nutrics is Project Progress Metrics
Project Progress Metrics 
Task Completion: Measures progress of tasks
Resource Usage: utilization of resource to ensure good allocation 
Test effort: overall effort invested in testing 
Test Progress Metrics: 
test Case implementation progress 
test environment Preparation progress
Number of test cases Run/not Run 
Number of test cases passed/Failed 
Test execution time
 Product Quality Metrics 
Availability. 
Response Time 
Mean Time to failure
Defect Metrics 
Number and Priorities of Defects Found/Fixed 
Defect Density = defects per unit 
Defect Detection percentage 
Risk Metrics 
Residual Risk Level = assesses level of risk that remains after mitigation efforts 
Coverage Metrics. 
Requirements coverage = percentage of requirements covered by test cases 
Code coverage = extent of code exercised by the test cases
Cost Metrics 
Cost of Testing 
organizational cost of quality 
Metrics assist in monitoring progress, assess quality, managing risks & making informed decisions
 Purpose, Content and Audience for Test Reports 
purposes of test Reporting 
Notifying project stakeholders about test results 
Help stakeholders understand & analyze results of a test period 
Help stakeholders to make decisions about how to guide the project forward 
Assuring that the original test plan will lead us to achieve testing goals 
Types of Test Reports: 
Test progress reports 
A test report prepared during a test activity 
supports the ongoing control at the testing 
key components in a test progress report: 
test period = Time of report and activities 
test progress = Summanizes overall progress of testing activities. 
Impediments for testing and their workaround = obstacles & how to our them 
test metrics 
New & changed risks  within the testing period 
Testing planned for the next period
Test completion reports 
summarizes the testing efforts & outcomes 
issues when exit criteria are reached
Elements of a test completion report: 
Test summary 
Testing & Product Quality Evaluation 
Deviations from the test plan 
Testing impediments & workarounds 
Test Metrics Based on test progress reports 
unmitigated risks and defects not fixed
 Communicating Status of Testing 
Means of communication should align w/ the organization test strategy 
There are regulatory standards some organization follows 
To also consider team members location 
communication options include: 
verbal communication = audience interaction 
dashboards = visual representation (example: burn-down charts which illustrate how work is decreasing overtime 
electronic communication channels=real-time updates 
online documentation = comprehensive information 
formal test reports = structured information which are structured documents that provide a detailed overview
 Configuration Management 
correct version of the software module?
version of the software System 
purpose is to establish and maintain integrity of the component or system 
version control 
test documentation 
should be identified during TEST PLANNING 
usually automated during DEVOPS

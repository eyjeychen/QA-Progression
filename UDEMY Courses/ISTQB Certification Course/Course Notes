July 5, 2025
Why is Testing Necessary?
 From the user’s perspective:
It is everywhere
Software that has minor issues might cause huge problems and frustrations
Can cause:
Money loss
Waste of time and effort
Business reputation
Loss of life
No issue is small

From the project management perspective:
It is a form of quality control

Typical Objectives in Testing:
Evaluating work products such as user requirements, user stories , designs and code
Causing failures and finding defects
Ensuring required coverage of a test object
Reducing the risk level of inadequate software quality
Verifying whether specified requirements have been fulfilled
Verifying that a test object complies with contractual, legal, and regulatory requirements
Providing information to stake holders to allow them to make informed decisions
Building confidence in the quality of a test object
Validating whether the test object is complete and works as expected by the stakeholders
Test objectives  are usually straight forward 
Test objectives display a list of time and ask what is a test objectives and what is not

Validation vs Verification
Validation mentions the user more and asks if we are building the right product
Verification mentions the documents more and asks if are we building the product right

Errors, Defects and Failures
A person makes an error that creates a defect / bug in the software that can cause a failure in the system operation
Example:
Someone has a fever (failure, a symptom you can see) and goes to the Dr who decides something is wrong with the stomach (a defect) because the patient ate too much (error)
Failures can also be caused by environmental conditions

False-positive vs False-negative
False-positive
We did something wrong which we thought we found a defect but it was a wrong finding
Probably duet to:
Errors in the test execution
Defects in the test data
Test environments
Other reasons
False-negative
We did something wrong but we cannot find the defect

Root Cause Analysis 
Helps in preventive measures for the software in the near future

Root Cause, Error, Defect, Failure
Root causes leads to human error which then leads to the defect thus the failure
Example:
The designer is tired (root cause)
Designer documents wrong for disabled users (error)
The programmers are in a severe time pressure (root cause)
So they did not include exception handling for calculations (error)

Dynamic and Static Testing
Dynamic testing is testing that occurs once the system is already there, so expect that there will be defects
Static testing takes place early in the software life cycle
Includes techniques such as reviewing documents and prevents defects from being introduced into the code

**July 6, 2025**

**Verification and Validation**

- Verification is whether the system the specified documents
- Validation checks whether the system will meet user and other stakeholder needs
- Verification + validation = doing the right thing in the right way

**What is Testing?**

- Testing is a big industry and has many branches
- Set of activities to discover defects and evaluate the quality of software artifacts
- **Artifacts are anything the development and testing teams produce to help create the software product**
- **Test objects are any artifact that is being tested**

**Testing and Debugging**

- Testing finds defects
- Debugging fixes these defects (a development activity)
- Debugging activities include:
    - Reproduction of a failure
    - Diagnosis (finding the root cause)
    - Fixing the cause
- Confirmation testing or re-testing is to test to check whether the bug was fixed
- Static testing points to the defect directly, debugging is concerned with removing it
    - No need for reproduction or diagnosis since static testing directly finds defects and cannot cause failures

**Does testing increase the quality of the software?**

- **No. Fixing bugs is a development activity not a testing activity. So testing simply builds confidence in the quality of the test object by providing higher-quality test objects**

**Testing’s Contributions to Success**

- Provides a cost-effective means of detecting defects

**Quality Assurance and testing**

- **Quality assurance -** “process-oriented”. The better the process, the better the software. “Process improvements”
    - If a good process is followed correctly, then it will generate a good product
    - A preventive approach
    - Test results in QA provide feedback on how well the development and processes perform
- **Testing -** “product-oriented”. It is a major form of **quality control**.
    - A corrective approach
    - Test results for QC are used to fix defects

**July 8, 2025**

**The Concept of Coverage in Software Testing**

- Test coverage is an essential part of software testing
- Measures the amount of testing performed by a set of tests
- Test coverage measures the effectiveness of our testing
- Parts we can measure for coverage
    - Requirements Coverage = test against all requirements
    - Structural coverage = has each design been exercised during testing
    - Implementation coverage = has each line of code been exercised?
- How can we know which requirement has been tested?
    - Traceability matrix

**The Seven Testing Principles**

1. Testing shows the presence of defects, not their absence
    1. There is no such thing as bug-free software
2. Exhaustive testing is impossible
    1. Testing everything is impossible except for trivial cases
3. Early Testing Saves Time and Money
    1. Early testing would be cheaper
    2. Time and Effort is reduced if discovered in the early phase
4. Defects Cluster Together
    1. A small number of modules usually contains most of the defects
    2. Closely related to the pareto principle (80/20 principle)
        1. 80 percent of the problems are usually found in 20% of the modules
5. Tests wear out
    1. If same tests are repeated, eventually the same set of test cases will no longer find any new defects
6. Testing is context-dependent
    1. Different testing is necessary for different circumstances
    2. Testing in an agile is different in a sequential
7. Absence of errors is a fallacy
    1. A mistake that there is no error present

**Test Conditions, Test Cases, Test procedure, and Test Suites**

- **Test Condition** is an item or event of a component that can be verified by one or more test cases.
    - Example: a function, transaction, feature, quality, etc
- **Test Cases is a set of input values, preconditions, expected results and post conditions developed from a test condition or objective**
    - **High-level test cases** mean they don't indicate exact data, just logical information
    - **Low-level test cases are also called concrete test cases (e.g. = test with input 10). Are test cases that contain data**
- **Test Procedure  - steps to execute a test case**
- **Test Suite -  Categorizing test procedures in such a way they match your planning and analysis needs.**

**Test Activities, Testware and Test Roles**

- ISO/IEC/IEEE 29119-2 describes the test processes in ISO Standard
- Testing is a process not simply just testing

**Test Activities and Tasks**

- Test process consists of the 7 groups of activities:
1. Test Planning
2. Test Monitoring and Control
3. Test Analysis
4. Test Design
5. Test Implementation
6. Test Execution
7. Test Completion
- Each activity may contain several more activities which may contain one or more tasks
- Test activities are organized and carried out differently in different life cycles
- Are usually implemented iteratively, not sequentially

**1. Test Planning**

- Define the objectives of testing
- Decide what to test
- Who will do the test
- How will they do the testing
- Define specific test activities to meet the objectives
- Define when we can consider the testing complete, called the **exit criteria**

**2. Test Monitoring and Control**

- The ongoing activity of comparing actual progress against the test plan using test monitoring metrics defined in the test plan
- Here is where we evaluate the exit criteria
- Evaluating exit criteria is the activity where test execution results are assessed against the defined objectives

**3. Test Analysis**

- Knowing what to test and breaking it into test conditions
- Any info or documentation we have is analyzed to identify testable features and define test conditions
- **Test Basis  - any document we can use as a reference**
    - Requirements specifications, such as BRD, SRS, user stories, use cases, etc.
    - Design and implementation information, UML, UI, etc
    - Code itself
    - Risk Analysis reports
- Identify features and sets of features to be tested
- Define and prioritize the test conditions for each features based on analysis
- Capture traceability = meaning we have test conditions for all features we have decided to test

**4. Test Design**

- Test design answers the question how to test
- Designing sets of test cases and prioritizing them
- Identifying test data to support test conditions
- Designing the test environment
- Capture bi-directional traceability

**5. Test Implementation**

- The testware necessary for test execution is created during test implementation
- “Do we now have everything in place to run the tests?”
- Create and implement test procedures during test implementation
- Prioritizing the test procedures
- Creating the test suites and arranging them
- Building the test environment
- Prepare and implement test data
- Verifying the bi-directional traceability

**Note:**

Test conditions = test analysis

Test Cases = test design

Test procedures = test implementation

Design data = test design

Implement the data = test implementation

**6. Test Execution**

- Test suites are run during the execution following the schedule
- Keeping a log of the testing and testware (pass or fail)
- Run test cases in order manually or automated
- Comparing actual results with expected results
- Analyzing anomalies when there is a difference between actual and expected results
- Reporting defects to devs for fix
- Confirmation testing
- Verifying and updating traceability

**7. Test Completion**

- Occur at project milestones
- Collect data from completed tests to consolidate experience
- Checking deliverables if they have been delivered
- Documentation is in order
- Create test summary report
- Check whether all defect reports are don
- Make sure delete confidential data
- Handing over the testware to the maintenance team
- Analyzing lessons
- Using data for improvement of test process maturity

**July 11, 2025**

**Test Process in Context**

- Not performed in isolation
- Organizational software development processes typically include testing activities
- Stakeholders provide financial funding for testing
- Test depends on context
- Factors Influencing Test Processes:
1. Stakeholders
2. Team Members
3. Business Domain
4. Technical Factors
5. Project Constraints
6. Organizational Factors
7. SDLC
8. Tools

**Testware**

- Any output work products like documents, reports, and lists created as part of the test processes
- Test process organization variation implementation:
1. Types of work products created per test process
2. Ways those work products are organized and managed
3. Names used for these work products
- Testing Standards:
1. ISO/IEC/IEEE-29119-1
- Software Testing Concepts
1. ISO/IEC/IEEE-29119-2

-	Software Testing Process

1. ISO/IEC/IEEE-29119-3
- Test Work Products
- 7 Different Work Products Per Different Test Process

**Traceability Between Test Basis and Test Work Products**

- Traceability of the test cases to requirements can verify if the requirements are covered by test cases
- Good Traceability Supports:
1. Analyzing Impact Changes
2. Meeting IT governance criteria
3. Making testing auditable
4. Improving the understandability of test progress reports and test summary reports
5. Relate IT terms and testing to stakeholders so they can understand them
6. Providing information about progress
- **Note:**
    - Traceability has nothing to do in any kind of estimation
    - Also has nothing to do with selecting data
    - It also cannot determine risk level using traceability

**Roles in Testing**

- 2 Main Roles
1. Test Manager
- Test process management
- Test planning and monitoring, etc
- “How-to-do” things
- **Test processes: Test planning, monitoring and control, Test completion**
- Test coach -> outside of the team
1. Tester
- Overall responsibility is the technical aspect of testing
- “Hands-on” to do things
- **Test processes: Test analysis, Test design, Test implementation, Test execution**

**Essential Skills and Good Practices in Testing**

- Knowledge + Practice + Aptitude (natural talent)
1. Testing Knowledge
2. Thoroughness, carefulness, etc, attention to detail
3. Good communication skills
4. Analytical Thinking
5. Technical Knowledge
6. Domain Knowledge

**Communication Skills for Testers**

- Developers treat their codes as if its their kids. They don't take criticism lightly

**Whole Team Approach**

- Ability for a tester to work in a team
- Contribute positively to team goals
- Any team member with the necessary knowledge and skills can perform any task and everyone is responsible for quality
- The team members share the same workspace
- Team takes responsibility in all kinds of testing tasks
- It generates team synergy that benefits the entire project
- Improve team dynamics
- Improve communication between team members

**Independence of Testing**

- Independence is not a replacement for familiarity
- Degrees of Independent Testing (from dependence 1 to independence 5)
1. The developer
2. Tester from the developer team
3. Independent test team
4. Customer of specialist testers
5. Outsourced test teams or testers
- Pros:
1. Independent testers are likely to recognize different kinds of failures compared to developers
2. They can verify, challenge or disprove assumptions
- Cons:
1. More isolation from the development team
2. Developers may lose a sense of responsibility for quality
3. Maybe seen for bottleneck or blamed for delays in release
4. May lack important information

July 13, 2025
Section 2 talks about:
How testing is incorporated in other dev activities
Different Levels and types of testing
Maintenance Testing
SDLC Models
How activities are organized to create the software
Vary according to:
Objective
Discipline
Interest
Time to market
Documentation
Types of activity performed at each stage
Others
Testing depends on the SDLC model
Categories of SDLC Models
Sequential Development Models
Linear with sequential flow of activities
No overlaps of phases
Iterative &Incremental Development Models
Short cycle of activities that can be revisited any time
Sequential Development Models: Waterfall
Oldest and well known
Each stage must be completed before moving on to the next stage
Requirements Phase
From the user
Systems specification document
Functional specifications
Design Phase
Global design or architecture design or high-level design to map the system requirements
Detailed design or low-level design: very specific about the functions
Coding Phase
Developers code the detailed design of each module to software
Testing Phase
Towards the end of the project lifecycle
*Waterfall model is only a good model for very specific circumstances
July 14, 2025
V-Model
A sequential model
Testing needs to begin as early as possible in the lifecycle
The model integrates testing throughout the entire development phase
The 4 Test Models in V-model
Component Testing
Search defects in implemented components
Integration Testing
Test interfaces and interactions between different components
System Testing
Behavior of the whole system
Acceptance Testing
Conducted whether or not to accept the system
E.g client UAT
**July 15, 2025**

**Iterative Incremental Development**

- Can be separated
- Incremental = building a system in pieces
- Iterative = built in a series of cycles where every piece can be used unlike incremental
- In iterative and incremental software, each cycle involved with everything we have in an SDLC
- Customers are heavily involved
- Examples of Iterative - Incremental models:
1. Rational Limited Process = each iterations are long and the increments are large
2. Spiral/Prototyping = creating experimental increments
3. Scrum = part of agile. Each iteration is short
4. Kanban = part of agile. Implemented with or without a fixed length of iterations
- Regression testing is increasingly important as the system grows
- Automation testing should be considered for faster testing in each cycle

Good Testing Practices in SDLC

1. In each development activity there is a corresponding testing activity
2. Each test level has specific objectives
3. Early testing in analysis and design; testing for every test level should begin early
4. Testers should be involved in reviewing documents or work products as soon as a draft is available

**What is Agile?**

- Ability to create and respond to change for good business development
- Ability to quickly reprioritize use of resources
- Not a project management methodology
- Not a development framework or lifecycle
- It is a philosophy
- It is lightweight

**Manifesto for Agile Software Development**

1. Individuals and interactions over process tools
2. Customer collaboration over contract negotiation
3. Working software over comprehensive documentation
4. Responding to change over following a plan

**Scrum**

- Most agile development framework
- Instruments and Practices of Scrum:
1. Sprints
- Fixed length
1. Product Increment
- Each sprint produces a product called an implement
1. Product Backlog
- Prioritized list of planned product increments in a log
1. Sprint Backlog
- Selects high priority list from product backlog and puts it into sprint backlog
1. Timeboxing
- Short fixed duration of periods of time in which activities are implemented
- Daily stand up meetings in 15 mins in 2 weeks (timeboxed)
1. Transparency
- Reporting of progress

Scrum Roles:

1. Scrum Master
- Like a project management head
- A coach
1. Product Owner
- Represents the customer which manages the product backlog
1. Development Team

**Impact of the Software Development Lifecycle of Testing**

1. Scope and timing of test activities
2. List of details of test documentation
3. Choice of test technologies and test approaches
4. Extent of test automation
5. Roles and responsibilities of a tester

**Testing as a Driver for Software Development**

- TDD, ATDD BDD approaches follows the test-first approach since the tests are defined before the code is written
- Supports software development module
1. TDD (Test-Driven Development)
- Tests are written before the code is written
- Directs coding through test cases instead of extensive software design
1. ATDD (Acceptance Test-Driven Development)
- Define the acceptance criteria and tests during the creation of user stories as part of the system design processes
1. BDD (Behavior Driven Development)
- Testing the code based on the expected behavior

Differences between the 3:

1. TDD
- Concentrates more on developer
- Developer makes unit tests to make sure the code works
1. ATDD
- Involves the customer from the beginning
1. BDD
- Defines the expected behavior of the system so anyone can understand
**July 16, 2025**

**DevOps and Testing**

- Isolation Issues:
1. Slow Software Delivery
2. Frequent errors and downtime
3. Inefficient processes
4. Limited visibility
5. Resistance to change
- Devops is a combination of development and operations
- CI/CD = continuous integration and continuous delivery
- Combination of these teams uses practices to automate processes
- Use a tech stack and tooling that helps them operate and evolve apps
- Introduces collaborative automated and approach to software delivery
- Promotes team autonomy, fast feedback, integrated tools and technical practices like CI and CD
- Benefits of Devops on Testing:
1. Fast feedback on the code quality
2. CI promotes shift-let approach
3. Automated Testing: devops promotes this
4. Increases view on non-functionality quality characteristics (performance)
5. Efficient regression testing
- Cons of Devops on Testing:
1. Devops delivery pipeline must be defined and establish
2. CI/CD tools must be introduced and maintained
3. Test automation requires additional resources

July 17, 2025
Shift-Left Approach
Principle of early testing
Does not wait for code to be implemented before testing
Shift-Left approach does the following:
Review specification
Write test cases before code is written
Continuous integration and continuous development
Begin implementing an automated process
Perform non-functional testing on component test-level
Retrospectives and Process Improvements
Does not include the end users
Retrospectives are frequent process improvements
Done at the end of each iteration
Notes what is and is not successful
Benefits of Retrospectives in Testing:
Increased effectiveness and efficiency
Increased quality of testing
Improved quality of test basis
Better collaboration
Test Levels
Groups of test activities
Unit testing-> component testing -> component integration testing -> system testing -> system integration testing -> acceptance testing
Characterizing Test Levels:
Specific Objectives
Test basis
Test objects
Specific approaches and responsibilities
Typical defects and failures
The 5 Main Test Levels:
Component Testing
Test components in a system
Testing unit per unit or component by component
Component Integration Testing
Testing interactions between components
System Testing
Overall behavior of the entire system
System Integration Testing
Several interfaces of a system
Acceptance Testing
Validation and readiness for deployment

**July 18, 2025**

**Component Testing**

- Lowest level of testing
- Small pieces of code developed by developers
- Unit or module testing
- Tested in isolation
- Done by developers
- Objectives of component testing
    - Reducing risk of delivery a bad component
    - Finding defects in the component
- Examples of Work Products that can be used as a test basis for component testing
    - Detailed design
    - Code
    - Data Model
    - Component Specifications
- A stub is a dummy code used in a place of a cold component
- A driver is a dummy code used in place of a collar component
- Examples of typical defects and failure in Component Testing
    - Incorrect code and logic
    - Data flow problems
    - Incorrect functionality

**Component Integration Testing**

- Putting units or components together to build the system
- Testers concentrate solely on the integration itself
- Examples of Work Products that can be used as a test basis for Component Integration Testing
    - Software and system design
    - Sequence diagrams
    - Uses cases
    - Architecture at component or system level
    - Workflow
- Usually part of the continuous integration process
- Focuses on the interactions and the interfaces between integrated components
- Usually carried out by developers and generally automated
- Examples of typical defects and failure in Component Integration Testing
    - incorrect or missing data
    - Incorrect sequencing or timing of interface calls
    - Interface mismatch
    - Failures in communication between components
- Larger integration or the “big bang” where all components are integrated can leave to problems as it is harder to isolate which component caused the defect
**July 19, 2025**

**System Testing**

- Whole system
- E2E testing both functional and non-functional
- Test environment should correspond to the final target or production environment
- Work Products used:
    - System Requirements Specification
    - Use Cases
    - Epics and user stories
    - Models and system behavior
    - Risk Analysis Reports
    - System and user manuals
- Test Objects
    - Applications
    - Hardware/Software Systems
    - Operating Systems
    - Systems Under Test (SUT)
    - System configuration and configuration data

**System Integration Testing**

- interactions /interfaces between systems
    - Example is web services or external system
- Objectives are the same for component integration testing

**Acceptance Testing**

- Yes or no testing; “should we release the system?”
- Behavior and capabilities of the whole system
- Produce information to assess the system’s readiness for deployment and use by the customer or end-users
- Typical Defects and Failures
    - System workflows do not meet user requirements
    - Business rules are not implemented correctly
    - Non-functional requirements
    - Does not satisfy contracted or regulatory requirements
- Common forms of Acceptance Testing
1. User Acceptance Test
- Intended users
1. Operational Acceptance Test
- System administrator’s point of view
1. Contractual and Regulatory Acceptance Test
- Users or independent users
- Government and legal regulations
1. Alpha and Beta Testing
- Alpha is inviting potential users in own developer site
- Beta testing is inviting potential users but they test in their own locations

**Test Types**

- Group of test activities aimed at testing specific characteristics of a software system
    - Functional Testing
    - Non-functional Testing
    - Black-box testing
    - White-box testing

**Functional Testing**

- “What” it does
- Should be performed at all test levels
- Objectives of functional testing
1. Functional Completeness
- Functions covers all the specified tasks and user objectives
1. Functional correctness
- Provide correct results
1. Functional appropriateness
- Fulfill the needs of the user

**Non-functional Testing**

- “How well it behaves”
- Eg. performance testing, security, maintenance, flexibility
- ISO/IEC 25010 for quality characteristics
- Performed at all test levels and is done as early as possible

**Black-box Testing**

- Specification-based testing
- Testing w/o looking at its internal code or implementation details
- Derives test cases and test scenarios from external documentation such as req. Specification and design documents

**White-box Testing**

- Testing w/ knowing how the system is structured
- Cover underlying structure of the system by tests

**Relationship b/w test types and test levels**

- Unnecessary for all software to have every test type represented
- Important to run appropriate test types at each level

**Confirmation and Regression Testing**

- Confirming if the defect has been fixed
- Regression testing helps to find out if the fix has introduced a new defect somewhere else
    - Test areas that was already tested
    - Performed at all test levels so its better to be automated
- Regression test sample:
- 

|  | EX 1 | EX 2 | EX 3 |
| --- | --- | --- | --- |
| TC 1 | (1) F | (4)P | (7) P |
| TC 2 | (2) P | (5)  F | (8)  P |
| TC3 | (3)F | (6)F | (9) P |
- Regression test: 5 and 7
- Confirmation test: 4, 6, 8, 9
- If we see a passed test and repeat it on the next execution then it is a **regression test**
- If we see a failed test then executed again then it is a **confirmation test**

**Maintenance Testing**

- Perform impact analysis before a change is made
- Takes place in a live system or a system in production
- Testing when any changes are made as part of maintenance

**Triggers for Maintenance and Maintenance Testing**

1. Modification
- Enhancement, upgrades
1. Upgrades or Migration
- One platform to another
- Data conversion
1. Retirement
- When an application reaches the end of its life

**Impact Analysis**

- Determining how the existing system maybe affected by changes
- Used to help decide how much regression testing is needed to do
- Identify impact of a change on existing tests

**July 22, 2025**

**Static Testing Basics**

- Is testing without executing code
- First line of defense against defects
- Relies on examining the software products
- Involves manual reviews and static analysis
- Static Analysis
    - Essential for safety-critical computer systems
    - Part of security testing
    - Incorporated into automated tests
    - Helps to identify problems prior to dynamic testing
- Static Analysis Test Objectives
    - Enhancing quality
    - Finding Bugs
    - Evaluating features
- Involves review techniques
    - Ensures user stories are complete and understandable

**Differences between static and dynamic testing**

- Static:
    - Finds defects
    - Improve consistency, and internal quality
    - Can be applied to non-executable work products
- Dynamic:
    - Finds failures
    - Focuses on externally visible behaviors
    - Can only be applied to executable code
- Typical Defects found in static testing:
1. Requirements Defects
2. Design Defects
3. Coding Defects
4. Deviations from standard
5. Incorrect interface specification
6. Security vulnerabilities
7. Gaps and inaccuracies in test basis
**July 23, 2025**

**Work Products that Can Be Examined by Static Testing**

- Any written work product can be reviewed by static testing
    - Specifications
    - Epics, user stories, acceptance criteria
    - Architecture and design specifications
    - Source codes
    - Testware
    - User guides
    - Web pages
    - contracts , project plans, etc
    - models
- A compiler - a static analysis tool
    - Example: grammar checking tool
- Some work products that are not understandable or readable by human language are not able to be identified by static testing
July 26, 2025 Notes
